# Learning notes on large scale optimization
## Stochastic Gradient Descent
>> For large neural networks with very large and highly redundant training sets, it is nearly always best to use mini-batch learning. -- Jeffery Hinton	


** Reference **
 - [Jeffery Hinton lecture notes in UToronto](http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)
 - [Aaditya Ramdas lecture video in CMU](https://www.youtube.com/watch?v=Cm90vyqQlFM&t=3498s)
 - [Ishan Misra lecture note in CMU](http://www.cs.cmu.edu/~imisra/data/Optimization_2015_11_11.pdf)
 - [Alex Smola lecture video in CMU](https://www.youtube.com/watch?v=Zm8l-JQAJD8)
 - [Ryan Tibshirani lecture video in CMU](https://www.youtube.com/watch?v=0CPmTogvICk)